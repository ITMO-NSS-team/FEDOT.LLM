{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_descriptions = {}\n",
    "big_descriptions[\"titanic\"] =  \"\"\"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\n",
    "\n",
    "In this competition, you’ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled train.csv and the other is titled test.csv.\n",
    "\n",
    "Train.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the “ground truth”.\n",
    "\n",
    "The test.csv dataset contains similar information but does not disclose the “ground truth” for each passenger. It’s your job to predict these outcomes.\n",
    "\n",
    "Using the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.\n",
    "\n",
    "Check out the “Data” tab to explore the datasets even further. Once you feel you’ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\n",
    "    \n",
    "    Dataset Description\n",
    "Overview\n",
    "The data has been split into two groups:\n",
    "\n",
    "training set (train.csv)\n",
    "test set (test.csv)\n",
    "The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n",
    "\n",
    "The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
    "\n",
    "We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_descriptions[\"credit-g\"] =  \"\"\"German Credit dataset\n",
    "This dataset classifies people described by a set of attributes as good or bad credit risks.\n",
    "\n",
    "This dataset comes with a cost matrix:\n",
    "\n",
    "Good  Bad (predicted)  \n",
    "Good   0    1   (actual)  \n",
    "Bad    5    0  \n",
    "It is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).\n",
    "\n",
    "Attribute description\n",
    "Status of existing checking account, in Deutsche Mark.\n",
    "Duration in months\n",
    "Credit history (credits taken, paid back duly, delays, critical accounts)\n",
    "Purpose of the credit (car, television,...)\n",
    "Credit amount\n",
    "Status of savings account/bonds, in Deutsche Mark.\n",
    "Present employment, in number of years.\n",
    "Installment rate in percentage of disposable income\n",
    "Personal status (married, single,...) and sex\n",
    "Other debtors / guarantors\n",
    "Present residence since X years\n",
    "Property (e.g. real estate)\n",
    "Age in years\n",
    "Other installment plans (banks, stores)\n",
    "Housing (rent, own,...)\n",
    "Number of existing credits at this bank\n",
    "Job\n",
    "Number of people being liable to provide maintenance for\n",
    "Telephone (yes,no)\n",
    "Foreign worker (yes,no)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_descriptions[\"playground-series-s3e23\"] =  \"\"\"Welcome to the 2023 edition of Kaggle's Playground Series!\n",
    "Thank you to everyone who participated in and contributed to Season 3 Playground Series so far!\n",
    "\n",
    "With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in October every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.\n",
    "\n",
    "Your Goal: Predict defects in C programs given various various attributes about the code.\n",
    "\n",
    "Dataset Description\n",
    "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Software Defect Dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n",
    "\n",
    "Files\n",
    "train.csv - the training dataset; defects is the binary target, which is treated as a boolean (False=0, True=1)\n",
    "test.csv - the test dataset; your objective is to predict the probability of positive defects (i.e., defects=True)\n",
    "sample_submission.csv - a sample submission file in the correct format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_descriptions[\"playground-series-s4e3\"] = \"\"\"Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting an approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.\n",
    "\n",
    "Your Goal: Predict the probability of various defects on steel plates. Good luck!\n",
    "\n",
    "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance.\n",
    "\n",
    "Files\n",
    "train.csv - the training dataset; there are 7 binary targets: Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, Other_Faults\n",
    "test.csv - the test dataset; your objective is to predict the probability of each of the 7 binary targets\n",
    "sample_submission.csv - a sample submission file in the correct format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_descriptions[\"steel-dataset\"] = \"\"\"Content\n",
    "This company produces several types of coils, steel plates, and iron plates. The information on electricity consumption is held in a cloud-based system. The information on energy consumption of the industry is stored on the website of the Korea Electric Power Corporation (pccs.kepco.go.kr), and the perspectives on daily, monthly, and annual data are calculated and shown.\n",
    "\n",
    "Attribute Information:\n",
    "Date Continuous-time data taken on the first of the month\n",
    "Usage_kWh Industry Energy Consumption Continuous kWh\n",
    "Lagging Current reactive power Continuous kVarh\n",
    "Leading Current reactive power Continuous kVarh\n",
    "CO2 Continuous ppm\n",
    "NSM Number of Seconds from midnight Continuous S\n",
    "Week status Categorical (Weekend (0) or a Weekday(1))\n",
    "Day of week Categorical Sunday, Monday : Saturday\n",
    "Load Type Categorical Light Load, Medium Load, Maximum Load\n",
    "\n",
    "Acknowledgements\n",
    "This dataset is sourced from the UCI Machine Learning Repository\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_descriptions[\"health_insurance\"] = \"\"\"Our client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.\n",
    "\n",
    "An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.\n",
    "\n",
    "For example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.\n",
    "\n",
    "Just like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called ‘sum assured’) to the customer.\n",
    "\n",
    "Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n",
    "\n",
    "Now, in order to predict, whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.\n",
    "\n",
    "Data Description\n",
    "Train Data\n",
    "Variable\tDefinition\n",
    "id\tUnique ID for the customer\n",
    "Gender\tGender of the customer\n",
    "Age\tAge of the customer\n",
    "Driving_License\t0 : Customer does not have DL, 1 : Customer already has DL\n",
    "Region_Code\tUnique code for the region of the customer\n",
    "Previously_Insured\t1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance\n",
    "Vehicle_Age\tAge of the Vehicle\n",
    "Vehicle_Damage\t1 : Customer got his/her vehicle damaged in the past. 0 : Customer didn't get his/her vehicle damaged in the past.\n",
    "Annual_Premium\tThe amount customer needs to pay as premium in the year\n",
    "Policy_Sales_Channel\tAnonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n",
    "Vintage\tNumber of Days, Customer has been associated with the company\n",
    "Response\t1 : Customer is interested, 0 : Customer is not interested\n",
    "Test Data\n",
    "Variable\tDefinition\n",
    "id\tUnique ID for the customer\n",
    "Gender\tGender of the customer\n",
    "Age\tAge of the customer\n",
    "Driving_License\t0 : Customer does not have DL, 1 : Customer already has DL\n",
    "Region_Code\tUnique code for the region of the customer\n",
    "Previously_Insured\t1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance\n",
    "Vehicle_Age\tAge of the Vehicle\n",
    "Vehicle_Damage\t1 : Customer got his/her vehicle damaged in the past. 0 : Customer didn't get his/her vehicle damaged in the past.\n",
    "Annual_Premium\tThe amount customer needs to pay as premium in the year\n",
    "Policy_Sales_Channel\tAnonymised Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n",
    "Vintage\tNumber of Days, Customer has been associated with the company\n",
    "Submission\n",
    "Variable\tDefinition\n",
    "id\tUnique ID for the customer\n",
    "Response\t1 : Customer is interested, 0 : Customer is not interested\n",
    "Evaluation Metric\n",
    "The evaluation metric for this hackathon is ROC_AUC score.\n",
    "\n",
    "Public and Private split\n",
    "The public leaderboard is based on 40% of test data, while final rank would be decided on remaining 60% of test data (which is private leaderboard)\n",
    "\n",
    "Guidelines for Final Submission\n",
    "Please ensure that your final submission includes the following:\n",
    "\n",
    "Solution file containing the predicted response of the customer (Probability of response 1)\n",
    "Code file for reproducing the submission, note that it is mandatory to submit your code for a valid final submission\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'big_descriptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../datasets/dataset_descriptions/big_descriptions.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m----> 3\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[43mbig_descriptions\u001b[49m, json_file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'big_descriptions' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../datasets/big_descriptions.json', \"w\") as json_file:\n",
    "    json.dump(big_descriptions, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
