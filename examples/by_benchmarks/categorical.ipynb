{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark for task type prediction using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.sep.join(['..', '..'])))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from fedot_llm.benchmarks import CategoricalBenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy mode (shortened to 2)\n",
    "Each description contains the most of provided data about each column. This is to show that model can confidently understand a well-described user task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets_metadata_path = '../../datasets/dataset_descriptions/categorical_benchmark/categorical_descriptions.json'\n",
    "datasets_metadata_path = '../../datasets/dataset_descriptions/categorical_benchmark/categorical_descriptions_short.json'\n",
    "\n",
    "benchmark = CategoricalBenchmark(\n",
    "    model=init_chat_model(\n",
    "        model=\"llama3.1\",\n",
    "        model_provider='ollama'),\n",
    "    datasets_metadata_path = datasets_metadata_path,\n",
    "    output='debug')\n",
    "predictions = await benchmark.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.display_results(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard mode \n",
    "Description only contains notes on data or vaguely describes each column, any explicit data about categoricality of columns was removed. It is expected that the model should understand categoricality mostly from minimal and undeited raw context and the processing of the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_metadata_path = '../../datasets/dataset_descriptions/categorical_benchmark/categorical_descriptions_hard.json'\n",
    "benchmark = CategoricalBenchmark(\n",
    "    model=init_chat_model(\n",
    "        model=\"llama3.1\",\n",
    "        model_provider='ollama'),\n",
    "    datasets_metadata_path = datasets_metadata_path,\n",
    "    output='debug')\n",
    "predictions = await benchmark.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.display_results(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
